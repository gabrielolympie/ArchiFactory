{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313550f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "## Auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import thunder\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "        \n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32\n",
    "print(f\"Using dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e8e295-e348-4c44-99c2-007b443c8214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import create_model_tokenizer, batch_tokenize, destruct_module_optimized\n",
    "from utils.lightning_utils import prepare_model_and_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6c243",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632de338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_accumulation_steps 6\n",
      "num_train_steps 10173\n",
      "num_train_samples 1953216\n",
      "num_test_samples 4096\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "dtype=torch.bfloat16\n",
    "\n",
    "# model and dataset\n",
    "model_id = 'unsloth/mistral-7b-instruct-v0.3'\n",
    "dataset_id = 'roneneldan/TinyStories'\n",
    "hidden_size = 512\n",
    "intermediate_size = 2048\n",
    "max_position_embeddings = 512\n",
    "text_field='text'\n",
    "\n",
    "## Dataset pre processing\n",
    "token_sample_estimate=10000\n",
    "quantile_treshold=0.95\n",
    "override_quantile_max_length=512 ## If quantile_max_length > override_quantile_max_length, cap it\n",
    "\n",
    "\n",
    "# train hp\n",
    "epochs=2\n",
    "batch_size=64\n",
    "max_length=256 # Enough to cover about 80% of the dataset\n",
    "learning_rate=2e-4\n",
    "min_lr=1e-6\n",
    "betas=(0.9, 0.999, 0.9999)\n",
    "alpha=5\n",
    "weight_decay=1e-4\n",
    "warmup_steps=64\n",
    "\n",
    "\n",
    "## Max length in tiny stories\n",
    "# Quantile : 0.1 Length : 142\n",
    "# Quantile : 0.25 Length : 164\n",
    "# Quantile : 0.5 Length : 191\n",
    "# Quantile : 0.75 Length : 227\n",
    "# Quantile : 0.9 Length : 306\n",
    "# Quantile : 0.99 Length : 605\n",
    "# Quantile : 0.95 Length : 423\n",
    "\n",
    "## Size the dataset, gradient_accumulation etc. based on token budget\n",
    "token_budget = 1_000_000_000\n",
    "batch_token_budget=100_000\n",
    "\n",
    "gradient_accumulation_steps=batch_token_budget // (batch_size * max_length)\n",
    "global_batch_size= gradient_accumulation_steps * batch_size\n",
    "\n",
    "num_train_steps = 1 + token_budget // ( max_length * batch_size * gradient_accumulation_steps)\n",
    "\n",
    "num_train_samples = num_train_steps * batch_size * gradient_accumulation_steps // epochs\n",
    "num_test_samples = batch_size * 64\n",
    "\n",
    "print('gradient_accumulation_steps', gradient_accumulation_steps)\n",
    "print('num_train_steps', num_train_steps)\n",
    "print('num_train_samples', num_train_samples)\n",
    "print('num_test_samples', num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219155f0-6bc4-4d39-863a-90e8aa7ac8bf",
   "metadata": {},
   "source": [
    "## Pre tokenize and pickle dataset (to run once each time you change tokenizer or dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83007f5-cb00-4333-a690-be4798746dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer, embed_tokens, lm_head, norm, vocab_size, hidden_size = create_model_tokenizer(model_id, load_model=False)\n",
    "# dataset = load_dataset(dataset_id)\n",
    "\n",
    "# sample = list(dataset['train'].select(range(token_sample_estimate))[text_field])\n",
    "\n",
    "# tokenized = batch_tokenize(\n",
    "#     tokenizer,\n",
    "#     sample,\n",
    "#     padding=None,\n",
    "#     batch_size=256,\n",
    "#     max_length=None\n",
    "# )\n",
    "\n",
    "# lens = [len(x) for x in tokenized]\n",
    "# quantiles_proba = [0.1, 0.25, 0.5, 0.75, 0.9, 0.99, quantile_treshold]\n",
    "# quantiles = list(map(lambda x:x+1, np.quantile(lens, q=quantiles_proba).tolist()))\n",
    "# quantile_max_length=int(quantiles[-1]) + 1\n",
    "\n",
    "# for q,l in zip(quantiles_proba, quantiles):\n",
    "#     print(f\"Quantile : {q} Length : {int(l)}\")\n",
    "\n",
    "# print(f'Calculated max_length for quantile {quantile_treshold} is {quantile_max_length}')\n",
    "\n",
    "# if quantile_max_length > override_quantile_max_length:\n",
    "#     quantile_max_length = override_quantile_max_length\n",
    "\n",
    "# train_set = batch_tokenize(tokenizer, list(dataset['train'][text_field]), batch_size=512, max_length=max_length)\n",
    "# val_set = batch_tokenize(tokenizer, list(dataset['validation'][text_field]), batch_size=64, max_length=max_length)\n",
    "\n",
    "# with open('/home/golympie/tokenized_dataset.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_set, val_set), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818c310-22ae-419c-95b8-5490bc60d6b0",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6675a40c-fb40-408d-ab98-cac8e237bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer, embed_tokens, lm_head, norm, vocab_size, hidden_size = create_model_tokenizer(model_id, load_model=False)\n",
    "\n",
    "with open('/home/golympie/tokenized_dataset.pickle', 'rb') as f:\n",
    "    train_set, test_set = pickle.load(f)\n",
    "\n",
    "train_set=train_set[:num_train_samples,:max_length]\n",
    "test_set=test_set[:num_test_samples,:max_length]\n",
    "    \n",
    "# train_set = train_set.to('cuda')\n",
    "# test_set = test_set.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbaba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partial train function\n",
    "def train(\n",
    "    model,\n",
    "    run_name,\n",
    "    do_compile=True,\n",
    "):\n",
    "    if do_compile:\n",
    "        model = torch.compile(\n",
    "            model,\n",
    "            # fullgraph=True,\n",
    "            # mode='max-autotune'\n",
    "        )\n",
    "        \n",
    "    lightning_model, trainer, train_loader, val_loader = prepare_model_and_data(\n",
    "        model,\n",
    "        train_data=train_set,\n",
    "        val_data=test_set,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        learning_rate=learning_rate,\n",
    "        betas=betas,\n",
    "        alpha=alpha,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        min_lr=min_lr,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        log_dir='runs',\n",
    "        log_name=run_name,\n",
    "        checkpoint_dir='checkpoints',\n",
    "        checkpoint_every_n_steps=200,\n",
    "    )\n",
    "    trainer.fit(\n",
    "        lightning_model,\n",
    "        train_loader,\n",
    "        val_loader\n",
    "    )\n",
    "    \n",
    "    # Save the model state\n",
    "    os.makedirs(\"states\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"states/{run_name}.pth\")\n",
    "    \n",
    "    destruct_module_optimized(model)\n",
    "    destruct_module_optimized(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53a2d8",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fb0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.archi_modules import StackedMixinForCausalLM, count_parameters\n",
    "from modules.positionnal_modules import NaivePositionnalEmbedding\n",
    "\n",
    "from modules.mixin_modules import (\n",
    "    RNNMixin,\n",
    "    LSTMMixin,\n",
    "    MultiScaleRetentionMixin,\n",
    "    Mamba2Mixin,\n",
    "    RWKV6Mixin,\n",
    "    GroupedQuerySelfAttentionMixin,\n",
    "    MultiHeadLatentAttentionMixin,\n",
    ")\n",
    "\n",
    "from modules.ffn_modules import FFN, SparseMoeFFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc39648",
   "metadata": {},
   "source": [
    "## STACK 8 - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2b82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "ffn_module = FFN(hidden_size, intermediate_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4127485",
   "metadata": {},
   "source": [
    "### GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gqsa = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=GroupedQuerySelfAttentionMixin(hidden_size, num_attention_heads=8, num_key_value_heads=4),\n",
    "    ffn_module=ffn_module,\n",
    "    positionnal_module=NaivePositionnalEmbedding(hidden_size, max_position_embeddings=max_position_embeddings)\n",
    ")\n",
    "\n",
    "count_parameters(gqsa)\n",
    "train(gqsa,run_name='gqsa', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d42d8",
   "metadata": {},
   "source": [
    "### Retentive Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "retnet = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=MultiScaleRetentionMixin(hidden_size, num_attention_heads=8, num_key_value_heads=1),\n",
    "    ffn_module=ffn_module,\n",
    "    positionnal_module=NaivePositionnalEmbedding(hidden_size, max_position_embeddings=max_position_embeddings)\n",
    ")\n",
    "\n",
    "count_parameters(retnet)\n",
    "train(retnet,run_name='retnet', do_compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6f066",
   "metadata": {},
   "source": [
    "### Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efab4c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/golympie/miniconda3/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:658: Checkpoint directory /mnt/c/Users/gabol/Desktop/ArchiFactory/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 65,383,680\n",
      "Embedding parameters: 16,777,216\n",
      "Mixin parameters: 6,626,048\n",
      "FFN parameters: 25,169,920\n",
      "LM Head parameters: 16,809,984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golympie/miniconda3/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:259: Found unsupported keys in the lr scheduler dict: {'start_epoch'}. HINT: remove them from the output of `configure_optimizers`.\n",
      "\n",
      "  | Name  | Type            | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model | OptimizedModule | 65.4 M | train\n",
      "--------------------------------------------------\n",
      "65.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "65.4 M    Total params\n",
      "261.535   Total estimated model params size (MB)\n",
      "136       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73517e15864944d3a7f5558d6c7ae643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9fcf5319414952bfcb88dc87de1354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                      | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596428f0ca1b4b3e8a2987c2faeee81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                    | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 28min 18s, sys: 16min 35s, total: 2h 44min 54s\n",
      "Wall time: 2h 51min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mamba = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=Mamba2Mixin(hidden_size = hidden_size, num_attention_heads=8),\n",
    "    ffn_module=ffn_module,\n",
    "    # positionnal_module=NaivePositionnalEmbedding(hidden_size, max_position_embeddings=max_position_embeddings)\n",
    ")\n",
    "\n",
    "count_parameters(mamba)\n",
    "train(mamba,run_name='mamba2', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d11bb",
   "metadata": {},
   "source": [
    "### RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad291e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golympie/miniconda3/lib/python3.11/site-packages/fla/layers/rwkv6.py:91: UserWarning: According to Bo, you are using a potentially buggy FLA implementation of RWKV. If you plan to report any numbers based on this implementation, we strongly recommend cross-checking with the official repo: https://github.com/BlinkDL/RWKV-LM. Bo may disagree with results reported from this version.\n",
      "  warnings.warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/golympie/miniconda3/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:658: Checkpoint directory /mnt/c/Users/gabol/Desktop/ArchiFactory/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 67,314,176\n",
      "Embedding parameters: 16,777,216\n",
      "Mixin parameters: 8,294,400\n",
      "FFN parameters: 25,169,920\n",
      "LM Head parameters: 16,809,984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golympie/miniconda3/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:259: Found unsupported keys in the lr scheduler dict: {'start_epoch'}. HINT: remove them from the output of `configure_optimizers`.\n",
      "\n",
      "  | Name  | Type            | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model | OptimizedModule | 67.3 M | train\n",
      "--------------------------------------------------\n",
      "67.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.3 M    Total params\n",
      "269.257   Total estimated model params size (MB)\n",
      "330       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7df6eee18f943c6b63afeb1f3ff5fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e427ee653254343b862de3a7efc6d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                      | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golympie/miniconda3/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py:1263: UserWarning: Dynamo does not know how to trace the builtin `cuda_utils.get_device_properties.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).\n",
      "If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.\n",
      "If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.\n",
      "  torch._dynamo.utils.warn_once(explanation + \"\\n\" + \"\\n\".join(hints))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rwkv = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=RWKV6Mixin(hidden_size = hidden_size, num_attention_heads=8),\n",
    "    ffn_module=ffn_module,\n",
    "    positionnal_module=NaivePositionnalEmbedding(hidden_size, max_position_embeddings=max_position_embeddings)\n",
    ")\n",
    "\n",
    "count_parameters(rwkv)\n",
    "train(rwkv,run_name='rwkv6', do_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9220d-082b-4c8d-a1f5-bd1a1513bf65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010f3f8-bdff-4150-966a-bbd0266c159d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5d82b-cbe3-447e-b39a-43da55da8579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06b981c5",
   "metadata": {},
   "source": [
    "## STACK 8 - Moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9471ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "ffn_module = SparseMoeFFN(\n",
    "    hidden_size,\n",
    "    hidden_size*4,\n",
    "    num_experts=8,\n",
    "    num_experts_per_tok=2,\n",
    "    norm_topk_prob=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b43ba",
   "metadata": {},
   "source": [
    "### GQA MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gqsa_moe = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=GroupedQuerySelfAttentionMixin(hidden_size, num_attention_heads=9, num_key_value_heads=9),\n",
    "    ffn_module=ffn_module,\n",
    "    positionnal_module=NaivePositionnalEmbedding(hidden_size, max_length=max_length)\n",
    ")\n",
    "\n",
    "count_parameters(gqsa_moe)\n",
    "train(gqsa_moe,run_name='gqsa-moe', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd0af3",
   "metadata": {},
   "source": [
    "### Retentive Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "retnet_moe = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=MultiScaleRetentionMixin(hidden_size, num_attention_heads=9, num_key_value_heads=3),\n",
    "    ffn_module=ffn_module,\n",
    "    # positionnal_module=NaivePositionnalEmbedding(hidden_size, max_length=max_length)\n",
    ")\n",
    "\n",
    "count_parameters(retnet_moe)\n",
    "train(retnet_moe,run_name='retnet-moe', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cc33b",
   "metadata": {},
   "source": [
    "### Mamba MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mamba_moe = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=Mamba2Mixin(hidden_size = hidden_size, num_attention_heads=6),\n",
    "    ffn_module=ffn_module,\n",
    "    # positionnal_module=NaivePositionnalEmbedding(hidden_size, max_length=max_length)\n",
    ")\n",
    "\n",
    "count_parameters(mamba_moe)\n",
    "train(mamba_moe,run_name='mamba-moe', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ab488",
   "metadata": {},
   "source": [
    "### RWKV MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rwkv_moe = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=RWKV6Mixin(hidden_size = hidden_size, num_attention_heads=9),\n",
    "    ffn_module=ffn_module,\n",
    "    # positionnal_module=NaivePositionnalEmbedding(hidden_size, max_length=max_length)\n",
    ")\n",
    "\n",
    "count_parameters(rwkv_moe)\n",
    "train(rwkv_moe,run_name='rwkv-moe', do_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3caec-54d3-46d1-94cb-e5096b687f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae90173-c7d2-4961-bd59-7632001cba02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e917bc-4467-435e-b833-40250f033cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
