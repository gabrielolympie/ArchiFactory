{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313550f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "## Auto reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import thunder\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "        \n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32\n",
    "print(f\"Using dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e8e295-e348-4c44-99c2-007b443c8214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import create_model_tokenizer, batch_tokenize, destruct_module_optimized\n",
    "from utils.lightning_utils import prepare_model_and_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6c243",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632de338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_accumulation_steps 48\n",
      "num_train_steps 10173\n",
      "num_train_samples 1953216\n",
      "num_test_samples 512\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "dtype=torch.bfloat16\n",
    "\n",
    "# model and dataset\n",
    "model_id = 'unsloth/mistral-7b-instruct-v0.3'\n",
    "dataset_id = 'roneneldan/TinyStories'\n",
    "hidden_size = 512\n",
    "intermediate_size = 2048\n",
    "max_position_embeddings = 512\n",
    "text_field='text'\n",
    "\n",
    "## Dataset pre processing\n",
    "token_sample_estimate=10000\n",
    "quantile_treshold=0.95\n",
    "override_quantile_max_length=512 ## If quantile_max_length > override_quantile_max_length, cap it\n",
    "\n",
    "\n",
    "# train hp\n",
    "epochs=2\n",
    "batch_size=8\n",
    "max_length=256 # Enough to cover about 80% of the dataset\n",
    "learning_rate=2e-4\n",
    "min_lr=1e-6\n",
    "betas=(0.9, 0.999, 0.9999)\n",
    "alpha=5\n",
    "weight_decay=1e-4\n",
    "warmup_steps=64\n",
    "warmup_steps=0\n",
    "\n",
    "\n",
    "## Max length in tiny stories\n",
    "# Quantile : 0.1 Length : 142\n",
    "# Quantile : 0.25 Length : 164\n",
    "# Quantile : 0.5 Length : 191\n",
    "# Quantile : 0.75 Length : 227\n",
    "# Quantile : 0.9 Length : 306\n",
    "# Quantile : 0.99 Length : 605\n",
    "# Quantile : 0.95 Length : 423\n",
    "\n",
    "## Size the dataset, gradient_accumulation etc. based on token budget\n",
    "token_budget = 1_000_000_000\n",
    "batch_token_budget=100_000\n",
    "\n",
    "gradient_accumulation_steps=batch_token_budget // (batch_size * max_length)\n",
    "global_batch_size= gradient_accumulation_steps * batch_size\n",
    "\n",
    "num_train_steps = 1 + token_budget // ( max_length * batch_size * gradient_accumulation_steps)\n",
    "\n",
    "num_train_samples = num_train_steps * batch_size * gradient_accumulation_steps // epochs\n",
    "num_test_samples = batch_size * 64\n",
    "\n",
    "print('gradient_accumulation_steps', gradient_accumulation_steps)\n",
    "print('num_train_steps', num_train_steps)\n",
    "print('num_train_samples', num_train_samples)\n",
    "print('num_test_samples', num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219155f0-6bc4-4d39-863a-90e8aa7ac8bf",
   "metadata": {},
   "source": [
    "## Pre tokenize and pickle dataset (to run once each time you change tokenizer or dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83007f5-cb00-4333-a690-be4798746dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer, embed_tokens, lm_head, norm, vocab_size, hidden_size = create_model_tokenizer(model_id, load_model=False)\n",
    "# dataset = load_dataset(dataset_id)\n",
    "\n",
    "# sample = list(dataset['train'].select(range(token_sample_estimate))[text_field])\n",
    "\n",
    "# tokenized = batch_tokenize(\n",
    "#     tokenizer,\n",
    "#     sample,\n",
    "#     padding=None,\n",
    "#     batch_size=256,\n",
    "#     max_length=None\n",
    "# )\n",
    "\n",
    "# lens = [len(x) for x in tokenized]\n",
    "# quantiles_proba = [0.1, 0.25, 0.5, 0.75, 0.9, 0.99, quantile_treshold]\n",
    "# quantiles = list(map(lambda x:x+1, np.quantile(lens, q=quantiles_proba).tolist()))\n",
    "# quantile_max_length=int(quantiles[-1]) + 1\n",
    "\n",
    "# for q,l in zip(quantiles_proba, quantiles):\n",
    "#     print(f\"Quantile : {q} Length : {int(l)}\")\n",
    "\n",
    "# print(f'Calculated max_length for quantile {quantile_treshold} is {quantile_max_length}')\n",
    "\n",
    "# if quantile_max_length > override_quantile_max_length:\n",
    "#     quantile_max_length = override_quantile_max_length\n",
    "\n",
    "# train_set = batch_tokenize(tokenizer, list(dataset['train'][text_field]), batch_size=512, max_length=max_length)\n",
    "# val_set = batch_tokenize(tokenizer, list(dataset['validation'][text_field]), batch_size=64, max_length=max_length)\n",
    "\n",
    "# with open('/home/golympie/tokenized_dataset.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_set, val_set), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818c310-22ae-419c-95b8-5490bc60d6b0",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6675a40c-fb40-408d-ab98-cac8e237bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer, embed_tokens, lm_head, norm, vocab_size, hidden_size = create_model_tokenizer(model_id, load_model=False)\n",
    "\n",
    "with open('/home/golympie/tokenized_dataset.pickle', 'rb') as f:\n",
    "    train_set, test_set = pickle.load(f)\n",
    "\n",
    "train_set=train_set[:num_train_samples,:max_length]\n",
    "test_set=test_set[:num_test_samples,:max_length]\n",
    "    \n",
    "# train_set = train_set.to('cuda')\n",
    "# test_set = test_set.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbaba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partial train function\n",
    "def train(\n",
    "    model,\n",
    "    run_name,\n",
    "    do_compile=True,\n",
    "):\n",
    "    if do_compile:\n",
    "        model = torch.compile(\n",
    "            model,\n",
    "            # fullgraph=True,\n",
    "            dynamic=True,\n",
    "            # mode='max-autotune'\n",
    "        )\n",
    "        \n",
    "    lightning_model, trainer, train_loader, val_loader = prepare_model_and_data(\n",
    "        model,\n",
    "        train_data=train_set,\n",
    "        val_data=test_set,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        learning_rate=learning_rate,\n",
    "        betas=betas,\n",
    "        alpha=alpha,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        min_lr=min_lr,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        log_dir='runs',\n",
    "        log_name=run_name,\n",
    "        checkpoint_dir='checkpoints',\n",
    "        checkpoint_every_n_steps=200,\n",
    "    )\n",
    "    trainer.fit(\n",
    "        lightning_model,\n",
    "        train_loader,\n",
    "        val_loader\n",
    "    )\n",
    "    \n",
    "    # Save the model state\n",
    "    os.makedirs(\"states\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"states/{run_name}.pth\")\n",
    "    \n",
    "    destruct_module_optimized(model)\n",
    "    destruct_module_optimized(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53a2d8",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92fb0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.archi_modules import StackedMixinForCausalLM, count_parameters\n",
    "from modules.positionnal_modules import NaivePositionnalEmbedding\n",
    "\n",
    "from modules.mixin_modules import (\n",
    "    RNNMixin,\n",
    "    LSTMMixin,\n",
    "    MultiScaleRetentionMixin,\n",
    "    Mamba2Mixin,\n",
    "    RWKV6Mixin,\n",
    "    GroupedQuerySelfAttentionMixin,\n",
    "    MultiHeadLatentAttentionMixin,\n",
    ")\n",
    "\n",
    "from modules.ffn_modules import FFN, SparseMoeFFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc39648",
   "metadata": {},
   "source": [
    "## STACK 8 - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2b82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "ffn_module = FFN(hidden_size, intermediate_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4127485",
   "metadata": {},
   "source": [
    "### GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gqsa = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=GroupedQuerySelfAttentionMixin(hidden_size, num_attention_heads=8, num_key_value_heads=4),\n",
    "    ffn_module=ffn_module,\n",
    "    positionnal_module=NaivePositionnalEmbedding(hidden_size, max_position_embeddings=max_position_embeddings)\n",
    ")\n",
    "\n",
    "count_parameters(gqsa)\n",
    "train(gqsa,run_name='gqsa', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d42d8",
   "metadata": {},
   "source": [
    "### Retentive Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "retnet = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=MultiScaleRetentionMixin(hidden_size, num_attention_heads=8, num_key_value_heads=1),\n",
    "    ffn_module=ffn_module,\n",
    "    positionnal_module=NaivePositionnalEmbedding(hidden_size, max_position_embeddings=max_position_embeddings)\n",
    ")\n",
    "\n",
    "count_parameters(retnet)\n",
    "train(retnet,run_name='retnet', do_compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6f066",
   "metadata": {},
   "source": [
    "### Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mamba = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=Mamba2Mixin(hidden_size = hidden_size, num_attention_heads=8),\n",
    "    ffn_module=ffn_module,\n",
    "    # positionnal_module=NaivePositionnalEmbedding(hidden_size, max_position_embeddings=max_position_embeddings)\n",
    ")\n",
    "\n",
    "count_parameters(mamba)\n",
    "train(mamba,run_name='mamba2', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d11bb",
   "metadata": {},
   "source": [
    "### RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad291e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rwkv = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=RWKV6Mixin(hidden_size = hidden_size, num_attention_heads=8),\n",
    "    ffn_module=ffn_module,\n",
    "    positionnal_module=NaivePositionnalEmbedding(hidden_size, max_position_embeddings=max_position_embeddings)\n",
    ")\n",
    "\n",
    "count_parameters(rwkv)\n",
    "train(rwkv,run_name='rwkv6', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065bb573-5089-4008-b76c-4b7c170691b7",
   "metadata": {},
   "source": [
    "## Private - This part will only work if you have access to the private part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010f3f8-bdff-4150-966a-bbd0266c159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/golympie/miniconda3/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:658: Checkpoint directory /mnt/c/Users/gabol/Desktop/ArchiFactory/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 63,540,480\n",
      "Embedding parameters: 16,777,216\n",
      "Mixin parameters: 0\n",
      "FFN parameters: 29,690,624\n",
      "LM Head parameters: 16,809,984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golympie/miniconda3/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:259: Found unsupported keys in the lr scheduler dict: {'start_epoch'}. HINT: remove them from the output of `configure_optimizers`.\n",
      "\n",
      "  | Name  | Type            | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model | OptimizedModule | 63.5 M | train\n",
      "--------------------------------------------------\n",
      "63.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "63.5 M    Total params\n",
      "254.162   Total estimated model params size (MB)\n",
      "78        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcf162e3cdf43699ba2b26bc161e015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1] Backend compiler exception\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]   Explanation: Backend compiler `inductor` failed with aten._local_scalar_dense.default\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1] \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]     While executing %item : [num_users=5] = call_method[target=item](args = (%getitem,), kwargs = {})\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]     GraphModule: class GraphModule(torch.nn.Module):\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         def forward(self, s3: \"Sym(s3)\", s4: \"Sym(s4)\", L_x_t_: \"f32[s3, 512][s4, 1]\", L_self_modules_fused_hyper_linear_parameters_weight_: \"f32[64000, 32][32, 1]\", L_self_modules_fused_hyper_linear_parameters_bias_: \"f32[64000][1]\", L_h_hyper_: \"f32[s3, 32][32, 1]\", L_self_buffers_fused_split_sizes_: \"i64[3][1]\", L_self_hidden_size: \"Sym(512)\", L_self_dora_rank: \"Sym(s7)\", L_self_intermediate_size: \"Sym(2048)\", L_self_modules_gate_proj_modules_linear_parameters_weight_: \"f32[2048, 512][512, 1]\", L_self_modules_gate_proj_in_features: \"Sym(512)\", L_norm_up_weight_: \"f32[2048, 512][512, 1]\", L_self_modules_up_proj_in_features: \"Sym(512)\", L_norm_down_weight_: \"f32[512, 2048][2048, 1]\", L_self_modules_down_proj_in_features: \"Sym(2048)\"):\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_x_t_ = L_x_t_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_fused_hyper_linear_parameters_weight_ = L_self_modules_fused_hyper_linear_parameters_weight_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_fused_hyper_linear_parameters_bias_ = L_self_modules_fused_hyper_linear_parameters_bias_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_h_hyper_ = L_h_hyper_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_buffers_fused_split_sizes_ = L_self_buffers_fused_split_sizes_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_hidden_size = L_self_hidden_size\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_dora_rank = L_self_dora_rank\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_intermediate_size = L_self_intermediate_size\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_gate_proj_modules_linear_parameters_weight_ = L_self_modules_gate_proj_modules_linear_parameters_weight_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_gate_proj_in_features = L_self_modules_gate_proj_in_features\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_norm_up_weight_ = L_norm_up_weight_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_up_proj_in_features = L_self_modules_up_proj_in_features\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_norm_down_weight_ = L_norm_down_weight_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_down_proj_in_features = L_self_modules_down_proj_in_features\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:207 in forward, code: x_t_unsqueezed = x_t.unsqueeze(1) # Shape: [B, 1, H]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             x_t_unsqueezed: \"f32[s3, 1, 512][s4, 512, 1]\" = l_x_t_.unsqueeze(1);  l_x_t_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:210 in forward, code: fused_params = self.fused_hyper_linear(h_hyper)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             fused_params: \"bf16[s3, 64000][64000, 1]\" = torch._C._nn.linear(l_h_hyper_, l_self_modules_fused_hyper_linear_parameters_weight_, l_self_modules_fused_hyper_linear_parameters_bias_);  l_h_hyper_ = l_self_modules_fused_hyper_linear_parameters_weight_ = l_self_modules_fused_hyper_linear_parameters_bias_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             getitem = l_self_buffers_fused_split_sizes_[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             item: \"Sym(u0)\" = getitem.item();  getitem = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _check_is_size = torch._check_is_size(item);  _check_is_size = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             ge: \"Sym(u0 >= 0)\" = item >= 0\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, \"Runtime assertion failed for expression u0 >= 0 on node 'ge'\");  ge = _assert_scalar_default = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             le: \"Sym(u0 <= 64000)\" = item <= 64000\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le, \"Runtime assertion failed for expression u0 <= 64000 on node 'le'\");  le = _assert_scalar_default_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             getitem_1 = l_self_buffers_fused_split_sizes_[1]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             item_1: \"Sym(2560*s7 + 2048)\" = getitem_1.item();  getitem_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _check_is_size_1 = torch._check_is_size(item_1);  _check_is_size_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             ge_1: \"Sym(2560*s7 + 2048 >= 4608)\" = item_1 >= 4608\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u1 >= 4608 on node 'ge_1'\");  ge_1 = _assert_scalar_default_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             le_1: \"Sym(2560*s7 + 2048 <= 64000)\" = item_1 <= 64000\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_3 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u1 <= 64000 on node 'le_1'\");  le_1 = _assert_scalar_default_3 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             eq: \"Sym(True)\" = item_1 == item_1\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_4 = torch.ops.aten._assert_scalar.default(eq, \"Runtime assertion failed for expression Eq(2560*s7 + 2048, u1) on node 'eq'\");  eq = _assert_scalar_default_4 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             getitem_2 = l_self_buffers_fused_split_sizes_[2];  l_self_buffers_fused_split_sizes_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             item_2: \"Sym(2560*s7 + 512)\" = getitem_2.item();  getitem_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _check_is_size_2 = torch._check_is_size(item_2);  _check_is_size_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             ge_2: \"Sym(2560*s7 + 512 >= 3072)\" = item_2 >= 3072\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_5 = torch.ops.aten._assert_scalar.default(ge_2, \"Runtime assertion failed for expression u2 >= 3072 on node 'ge_2'\");  ge_2 = _assert_scalar_default_5 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             le_2: \"Sym(2560*s7 + 512 <= 61952)\" = item_2 <= 61952\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_6 = torch.ops.aten._assert_scalar.default(le_2, \"Runtime assertion failed for expression u2 <= 61952 on node 'le_2'\");  le_2 = _assert_scalar_default_6 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             sym_sum: \"Sym(5120*s7 + u0 + 2560)\" = torch.sym_sum([item, item_1, item_2])\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             eq_1: \"Sym(Eq(5120*s7 + u0 + 2560, 64000))\" = sym_sum == 64000;  sym_sum = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_7 = torch.ops.aten._assert_scalar.default(eq_1, \"Runtime assertion failed for expression Eq(u0 + u1 + u2, 64000) on node 'eq_1'\");  eq_1 = _assert_scalar_default_7 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             eq_2: \"Sym(True)\" = item_2 == item_2\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_8 = torch.ops.aten._assert_scalar.default(eq_2, \"Runtime assertion failed for expression Eq(2560*s7 + 512, u2) on node 'eq_2'\");  eq_2 = _assert_scalar_default_8 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             split = torch.functional.split(fused_params, [item, item_1, item_2], dim = -1);  fused_params = item = item_1 = item_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             gate_params: \"bf16[s3, u0][64000, 1]\" = split[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             up_params: \"bf16[s3, 2560*s7 + 2048][64000, 1]\" = split[1]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             down_params: \"bf16[s3, 2560*s7 + 512][64000, 1]\" = split[2];  split = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:214 in forward, code: lora_a_gate, lora_b_gate = gate_params.split([self.hidden_size * self.dora_rank, self.intermediate_size * self.dora_rank], dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             mul: \"Sym(512*s7)\" = l_self_hidden_size * l_self_dora_rank\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             mul_1: \"Sym(2048*s7)\" = l_self_intermediate_size * l_self_dora_rank\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             split_1 = gate_params.split([mul, mul_1], dim = -1);  gate_params = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_gate: \"bf16[s3, 512*s7][64000, 1]\" = split_1[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_gate: \"bf16[s3, 2048*s7][64000, 1]\" = split_1[1];  split_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:216 in forward, code: lora_a_gate.view(batch_size, self.dora_rank, self.hidden_size),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_new: \"bf16[s3, s7, 512][64000, 512, 1]\" = lora_a_gate.view(s3, l_self_dora_rank, l_self_hidden_size);  lora_a_gate = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:217 in forward, code: lora_b_gate.view(batch_size, self.intermediate_size, self.dora_rank),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_new: \"bf16[s3, 2048, s7][64000, s7, 1]\" = lora_b_gate.view(s3, l_self_intermediate_size, l_self_dora_rank);  lora_b_gate = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:130 in forward, code: base_output = self.linear(x)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch._C._nn.linear(x_t_unsqueezed, l_self_modules_gate_proj_modules_linear_parameters_weight_, None);  l_self_modules_gate_proj_modules_linear_parameters_weight_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             x_reshaped: \"f32[s3, 1, 512][s4, 512, 1]\" = x_t_unsqueezed.view(s3, 1, l_self_modules_gate_proj_in_features);  l_self_modules_gate_proj_in_features = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a: \"bf16[s3, 512, s7][64000, 1, 512]\" = lora_a_new.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b: \"bf16[s3, s7, 2048][64000, 1, s7]\" = lora_b_new.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             bmm: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped, lora_a);  x_reshaped = lora_a = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_term: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch.bmm(bmm, lora_b);  bmm = lora_b = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output + lora_term;  base_output = lora_term = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:220 in forward, code: gate = self.gate_proj(x_t_unsqueezed, precomputed_params=precomp_gate).squeeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             gate: \"bf16[s3, 2048][2048, 1]\" = add.squeeze(1);  add = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:223 in forward, code: lora_a_up, lora_b_up, dora_mag_up = up_params.split([self.hidden_size * self.dora_rank, self.intermediate_size * self.dora_rank, self.intermediate_size], dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             split_2 = up_params.split([mul, mul_1, l_self_intermediate_size], dim = -1);  up_params = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_up: \"bf16[s3, 512*s7][64000, 1]\" = split_2[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_up: \"bf16[s3, 2048*s7][64000, 1]\" = split_2[1]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             dora_mag_up: \"bf16[s3, 2048][64000, 1]\" = split_2[2];  split_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:225 in forward, code: lora_a_up.view(batch_size, self.dora_rank, self.hidden_size),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_new_1: \"bf16[s3, s7, 512][64000, 512, 1]\" = lora_a_up.view(s3, l_self_dora_rank, l_self_hidden_size);  lora_a_up = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:226 in forward, code: lora_b_up.view(batch_size, self.intermediate_size, self.dora_rank),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_new_1: \"bf16[s3, 2048, s7][64000, s7, 1]\" = lora_b_up.view(s3, l_self_intermediate_size, l_self_dora_rank);  lora_b_up = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:227 in forward, code: dora_mag_up.view(batch_size, self.intermediate_size)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             dora_magnitude_new: \"bf16[s3, 2048][64000, 1]\" = dora_mag_up.view(s3, l_self_intermediate_size);  dora_mag_up = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:141 in forward, code: base_output_unscaled = F.linear(x, weight_normalized)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output_unscaled: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch._C._nn.linear(x_t_unsqueezed, l_norm_up_weight_);  l_norm_up_weight_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:142 in forward, code: base_output = base_output_unscaled * (1.0 + self.dora_magnitude_ema.unsqueeze(1))\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             unsqueeze_1: \"bf16[s3, 1, 2048][64000, 2048, 1]\" = dora_magnitude_new.unsqueeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = 1.0 + unsqueeze_1;  unsqueeze_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output_unscaled * add_1;  base_output_unscaled = add_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             x_reshaped_1: \"f32[s3, 1, 512][s4, 512, 1]\" = x_t_unsqueezed.view(s3, 1, l_self_modules_up_proj_in_features);  x_t_unsqueezed = l_self_modules_up_proj_in_features = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_1: \"bf16[s3, 512, s7][64000, 1, 512]\" = lora_a_new_1.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_1: \"bf16[s3, s7, 2048][64000, 1, s7]\" = lora_b_new_1.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             bmm_2: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped_1, lora_a_1);  x_reshaped_1 = lora_a_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_term_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch.bmm(bmm_2, lora_b_1);  bmm_2 = lora_b_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add_2: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output_1 + lora_term_1;  base_output_1 = lora_term_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:229 in forward, code: up = self.up_proj(x_t_unsqueezed, precomputed_params=precomp_up, precomputed_norm_weight=norm_up_weight).squeeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             up: \"bf16[s3, 2048][2048, 1]\" = add_2.squeeze(1);  add_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:232 in forward, code: intermediate = self.act_fn(gate) * up\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             silu: \"bf16[s3, 2048][2048, 1]\" = torch.nn.functional.silu(gate, inplace = False);  gate = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             intermediate: \"bf16[s3, 2048][2048, 1]\" = silu * up;  silu = up = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:233 in forward, code: intermediate_unsqueezed = intermediate.unsqueeze(1) # Shape: [B, 1, I]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             intermediate_unsqueezed: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = intermediate.unsqueeze(1);  intermediate = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:236 in forward, code: lora_a_down, lora_b_down, dora_mag_down = down_params.split([self.intermediate_size * self.dora_rank, self.hidden_size * self.dora_rank, self.hidden_size], dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             split_3 = down_params.split([mul_1, mul, l_self_hidden_size], dim = -1);  down_params = mul_1 = mul = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_down: \"bf16[s3, 2048*s7][64000, 1]\" = split_3[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_down: \"bf16[s3, 512*s7][64000, 1]\" = split_3[1]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             dora_mag_down: \"bf16[s3, 512][64000, 1]\" = split_3[2];  split_3 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:238 in forward, code: lora_a_down.view(batch_size, self.dora_rank, self.intermediate_size),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_new_2: \"bf16[s3, s7, 2048][64000, 2048, 1]\" = lora_a_down.view(s3, l_self_dora_rank, l_self_intermediate_size);  lora_a_down = l_self_intermediate_size = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:239 in forward, code: lora_b_down.view(batch_size, self.hidden_size, self.dora_rank),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_new_2: \"bf16[s3, 512, s7][64000, s7, 1]\" = lora_b_down.view(s3, l_self_hidden_size, l_self_dora_rank);  lora_b_down = l_self_dora_rank = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:240 in forward, code: dora_mag_down.view(batch_size, self.hidden_size)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             dora_magnitude_new_1: \"bf16[s3, 512][64000, 1]\" = dora_mag_down.view(s3, l_self_hidden_size);  dora_mag_down = l_self_hidden_size = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:141 in forward, code: base_output_unscaled = F.linear(x, weight_normalized)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output_unscaled_1: \"bf16[s3, 1, 512][512, 512, 1]\" = torch._C._nn.linear(intermediate_unsqueezed, l_norm_down_weight_);  l_norm_down_weight_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:142 in forward, code: base_output = base_output_unscaled * (1.0 + self.dora_magnitude_ema.unsqueeze(1))\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             unsqueeze_3: \"bf16[s3, 1, 512][64000, 512, 1]\" = dora_magnitude_new_1.unsqueeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add_3: \"bf16[s3, 1, 512][512, 512, 1]\" = 1.0 + unsqueeze_3;  unsqueeze_3 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output_2: \"bf16[s3, 1, 512][512, 512, 1]\" = base_output_unscaled_1 * add_3;  base_output_unscaled_1 = add_3 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             x_reshaped_2: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = intermediate_unsqueezed.view(s3, 1, l_self_modules_down_proj_in_features);  intermediate_unsqueezed = s3 = l_self_modules_down_proj_in_features = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_2: \"bf16[s3, 2048, s7][64000, 1, 2048]\" = lora_a_new_2.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_2: \"bf16[s3, s7, 512][64000, 1, s7]\" = lora_b_new_2.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             bmm_4: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped_2, lora_a_2);  x_reshaped_2 = lora_a_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_term_2: \"bf16[s3, 1, 512][512, 512, 1]\" = torch.bmm(bmm_4, lora_b_2);  bmm_4 = lora_b_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add_4: \"bf16[s3, 1, 512][512, 512, 1]\" = base_output_2 + lora_term_2;  base_output_2 = lora_term_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:242 in forward, code: down = self.down_proj(intermediate_unsqueezed, precomputed_params=precomp_down, precomputed_norm_weight=norm_down_weight).squeeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             down: \"bf16[s3, 512][512, 1]\" = add_4.squeeze(1);  add_4 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             return (down, lora_a_new, lora_b_new, lora_a_new_1, lora_b_new_1, dora_magnitude_new, lora_a_new_2, lora_b_new_2, dora_magnitude_new_1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1] \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]     Original traceback:\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]       File \"/mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py\", line 211, in forward\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]     . Adding a graph break.\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]   Hint: Report an issue to the backend compiler repo.\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1] \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]   Developer debug context: Backend: inductor\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]     Exception:aten._local_scalar_dense.default\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1] \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]     While executing %item : [num_users=5] = call_method[target=item](args = (%getitem,), kwargs = {})\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]     GraphModule: class GraphModule(torch.nn.Module):\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         def forward(self, s3: \"Sym(s3)\", s4: \"Sym(s4)\", L_x_t_: \"f32[s3, 512][s4, 1]\", L_self_modules_fused_hyper_linear_parameters_weight_: \"f32[64000, 32][32, 1]\", L_self_modules_fused_hyper_linear_parameters_bias_: \"f32[64000][1]\", L_h_hyper_: \"f32[s3, 32][32, 1]\", L_self_buffers_fused_split_sizes_: \"i64[3][1]\", L_self_hidden_size: \"Sym(512)\", L_self_dora_rank: \"Sym(s7)\", L_self_intermediate_size: \"Sym(2048)\", L_self_modules_gate_proj_modules_linear_parameters_weight_: \"f32[2048, 512][512, 1]\", L_self_modules_gate_proj_in_features: \"Sym(512)\", L_norm_up_weight_: \"f32[2048, 512][512, 1]\", L_self_modules_up_proj_in_features: \"Sym(512)\", L_norm_down_weight_: \"f32[512, 2048][2048, 1]\", L_self_modules_down_proj_in_features: \"Sym(2048)\"):\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_x_t_ = L_x_t_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_fused_hyper_linear_parameters_weight_ = L_self_modules_fused_hyper_linear_parameters_weight_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_fused_hyper_linear_parameters_bias_ = L_self_modules_fused_hyper_linear_parameters_bias_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_h_hyper_ = L_h_hyper_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_buffers_fused_split_sizes_ = L_self_buffers_fused_split_sizes_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_hidden_size = L_self_hidden_size\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_dora_rank = L_self_dora_rank\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_intermediate_size = L_self_intermediate_size\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_gate_proj_modules_linear_parameters_weight_ = L_self_modules_gate_proj_modules_linear_parameters_weight_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_gate_proj_in_features = L_self_modules_gate_proj_in_features\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_norm_up_weight_ = L_norm_up_weight_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_up_proj_in_features = L_self_modules_up_proj_in_features\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_norm_down_weight_ = L_norm_down_weight_\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             l_self_modules_down_proj_in_features = L_self_modules_down_proj_in_features\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:207 in forward, code: x_t_unsqueezed = x_t.unsqueeze(1) # Shape: [B, 1, H]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             x_t_unsqueezed: \"f32[s3, 1, 512][s4, 512, 1]\" = l_x_t_.unsqueeze(1);  l_x_t_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:210 in forward, code: fused_params = self.fused_hyper_linear(h_hyper)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             fused_params: \"bf16[s3, 64000][64000, 1]\" = torch._C._nn.linear(l_h_hyper_, l_self_modules_fused_hyper_linear_parameters_weight_, l_self_modules_fused_hyper_linear_parameters_bias_);  l_h_hyper_ = l_self_modules_fused_hyper_linear_parameters_weight_ = l_self_modules_fused_hyper_linear_parameters_bias_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             getitem = l_self_buffers_fused_split_sizes_[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             item: \"Sym(u0)\" = getitem.item();  getitem = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _check_is_size = torch._check_is_size(item);  _check_is_size = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             ge: \"Sym(u0 >= 0)\" = item >= 0\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, \"Runtime assertion failed for expression u0 >= 0 on node 'ge'\");  ge = _assert_scalar_default = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             le: \"Sym(u0 <= 64000)\" = item <= 64000\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le, \"Runtime assertion failed for expression u0 <= 64000 on node 'le'\");  le = _assert_scalar_default_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             getitem_1 = l_self_buffers_fused_split_sizes_[1]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             item_1: \"Sym(2560*s7 + 2048)\" = getitem_1.item();  getitem_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _check_is_size_1 = torch._check_is_size(item_1);  _check_is_size_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             ge_1: \"Sym(2560*s7 + 2048 >= 4608)\" = item_1 >= 4608\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u1 >= 4608 on node 'ge_1'\");  ge_1 = _assert_scalar_default_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             le_1: \"Sym(2560*s7 + 2048 <= 64000)\" = item_1 <= 64000\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_3 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u1 <= 64000 on node 'le_1'\");  le_1 = _assert_scalar_default_3 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             eq: \"Sym(True)\" = item_1 == item_1\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_4 = torch.ops.aten._assert_scalar.default(eq, \"Runtime assertion failed for expression Eq(2560*s7 + 2048, u1) on node 'eq'\");  eq = _assert_scalar_default_4 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             getitem_2 = l_self_buffers_fused_split_sizes_[2];  l_self_buffers_fused_split_sizes_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             item_2: \"Sym(2560*s7 + 512)\" = getitem_2.item();  getitem_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _check_is_size_2 = torch._check_is_size(item_2);  _check_is_size_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             ge_2: \"Sym(2560*s7 + 512 >= 3072)\" = item_2 >= 3072\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_5 = torch.ops.aten._assert_scalar.default(ge_2, \"Runtime assertion failed for expression u2 >= 3072 on node 'ge_2'\");  ge_2 = _assert_scalar_default_5 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             le_2: \"Sym(2560*s7 + 512 <= 61952)\" = item_2 <= 61952\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_6 = torch.ops.aten._assert_scalar.default(le_2, \"Runtime assertion failed for expression u2 <= 61952 on node 'le_2'\");  le_2 = _assert_scalar_default_6 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             sym_sum: \"Sym(5120*s7 + u0 + 2560)\" = torch.sym_sum([item, item_1, item_2])\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             eq_1: \"Sym(Eq(5120*s7 + u0 + 2560, 64000))\" = sym_sum == 64000;  sym_sum = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_7 = torch.ops.aten._assert_scalar.default(eq_1, \"Runtime assertion failed for expression Eq(u0 + u1 + u2, 64000) on node 'eq_1'\");  eq_1 = _assert_scalar_default_7 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             eq_2: \"Sym(True)\" = item_2 == item_2\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             _assert_scalar_default_8 = torch.ops.aten._assert_scalar.default(eq_2, \"Runtime assertion failed for expression Eq(2560*s7 + 512, u2) on node 'eq_2'\");  eq_2 = _assert_scalar_default_8 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             split = torch.functional.split(fused_params, [item, item_1, item_2], dim = -1);  fused_params = item = item_1 = item_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             gate_params: \"bf16[s3, u0][64000, 1]\" = split[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             up_params: \"bf16[s3, 2560*s7 + 2048][64000, 1]\" = split[1]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             down_params: \"bf16[s3, 2560*s7 + 512][64000, 1]\" = split[2];  split = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:214 in forward, code: lora_a_gate, lora_b_gate = gate_params.split([self.hidden_size * self.dora_rank, self.intermediate_size * self.dora_rank], dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             mul: \"Sym(512*s7)\" = l_self_hidden_size * l_self_dora_rank\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             mul_1: \"Sym(2048*s7)\" = l_self_intermediate_size * l_self_dora_rank\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             split_1 = gate_params.split([mul, mul_1], dim = -1);  gate_params = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_gate: \"bf16[s3, 512*s7][64000, 1]\" = split_1[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_gate: \"bf16[s3, 2048*s7][64000, 1]\" = split_1[1];  split_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:216 in forward, code: lora_a_gate.view(batch_size, self.dora_rank, self.hidden_size),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_new: \"bf16[s3, s7, 512][64000, 512, 1]\" = lora_a_gate.view(s3, l_self_dora_rank, l_self_hidden_size);  lora_a_gate = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:217 in forward, code: lora_b_gate.view(batch_size, self.intermediate_size, self.dora_rank),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_new: \"bf16[s3, 2048, s7][64000, s7, 1]\" = lora_b_gate.view(s3, l_self_intermediate_size, l_self_dora_rank);  lora_b_gate = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:130 in forward, code: base_output = self.linear(x)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch._C._nn.linear(x_t_unsqueezed, l_self_modules_gate_proj_modules_linear_parameters_weight_, None);  l_self_modules_gate_proj_modules_linear_parameters_weight_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             x_reshaped: \"f32[s3, 1, 512][s4, 512, 1]\" = x_t_unsqueezed.view(s3, 1, l_self_modules_gate_proj_in_features);  l_self_modules_gate_proj_in_features = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a: \"bf16[s3, 512, s7][64000, 1, 512]\" = lora_a_new.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b: \"bf16[s3, s7, 2048][64000, 1, s7]\" = lora_b_new.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             bmm: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped, lora_a);  x_reshaped = lora_a = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_term: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch.bmm(bmm, lora_b);  bmm = lora_b = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output + lora_term;  base_output = lora_term = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:220 in forward, code: gate = self.gate_proj(x_t_unsqueezed, precomputed_params=precomp_gate).squeeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             gate: \"bf16[s3, 2048][2048, 1]\" = add.squeeze(1);  add = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:223 in forward, code: lora_a_up, lora_b_up, dora_mag_up = up_params.split([self.hidden_size * self.dora_rank, self.intermediate_size * self.dora_rank, self.intermediate_size], dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             split_2 = up_params.split([mul, mul_1, l_self_intermediate_size], dim = -1);  up_params = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_up: \"bf16[s3, 512*s7][64000, 1]\" = split_2[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_up: \"bf16[s3, 2048*s7][64000, 1]\" = split_2[1]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             dora_mag_up: \"bf16[s3, 2048][64000, 1]\" = split_2[2];  split_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:225 in forward, code: lora_a_up.view(batch_size, self.dora_rank, self.hidden_size),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_new_1: \"bf16[s3, s7, 512][64000, 512, 1]\" = lora_a_up.view(s3, l_self_dora_rank, l_self_hidden_size);  lora_a_up = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:226 in forward, code: lora_b_up.view(batch_size, self.intermediate_size, self.dora_rank),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_new_1: \"bf16[s3, 2048, s7][64000, s7, 1]\" = lora_b_up.view(s3, l_self_intermediate_size, l_self_dora_rank);  lora_b_up = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:227 in forward, code: dora_mag_up.view(batch_size, self.intermediate_size)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             dora_magnitude_new: \"bf16[s3, 2048][64000, 1]\" = dora_mag_up.view(s3, l_self_intermediate_size);  dora_mag_up = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:141 in forward, code: base_output_unscaled = F.linear(x, weight_normalized)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output_unscaled: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch._C._nn.linear(x_t_unsqueezed, l_norm_up_weight_);  l_norm_up_weight_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:142 in forward, code: base_output = base_output_unscaled * (1.0 + self.dora_magnitude_ema.unsqueeze(1))\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             unsqueeze_1: \"bf16[s3, 1, 2048][64000, 2048, 1]\" = dora_magnitude_new.unsqueeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = 1.0 + unsqueeze_1;  unsqueeze_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output_unscaled * add_1;  base_output_unscaled = add_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             x_reshaped_1: \"f32[s3, 1, 512][s4, 512, 1]\" = x_t_unsqueezed.view(s3, 1, l_self_modules_up_proj_in_features);  x_t_unsqueezed = l_self_modules_up_proj_in_features = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_1: \"bf16[s3, 512, s7][64000, 1, 512]\" = lora_a_new_1.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_1: \"bf16[s3, s7, 2048][64000, 1, s7]\" = lora_b_new_1.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             bmm_2: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped_1, lora_a_1);  x_reshaped_1 = lora_a_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_term_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch.bmm(bmm_2, lora_b_1);  bmm_2 = lora_b_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add_2: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output_1 + lora_term_1;  base_output_1 = lora_term_1 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:229 in forward, code: up = self.up_proj(x_t_unsqueezed, precomputed_params=precomp_up, precomputed_norm_weight=norm_up_weight).squeeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             up: \"bf16[s3, 2048][2048, 1]\" = add_2.squeeze(1);  add_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:232 in forward, code: intermediate = self.act_fn(gate) * up\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             silu: \"bf16[s3, 2048][2048, 1]\" = torch.nn.functional.silu(gate, inplace = False);  gate = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             intermediate: \"bf16[s3, 2048][2048, 1]\" = silu * up;  silu = up = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:233 in forward, code: intermediate_unsqueezed = intermediate.unsqueeze(1) # Shape: [B, 1, I]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             intermediate_unsqueezed: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = intermediate.unsqueeze(1);  intermediate = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:236 in forward, code: lora_a_down, lora_b_down, dora_mag_down = down_params.split([self.intermediate_size * self.dora_rank, self.hidden_size * self.dora_rank, self.hidden_size], dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             split_3 = down_params.split([mul_1, mul, l_self_hidden_size], dim = -1);  down_params = mul_1 = mul = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_down: \"bf16[s3, 2048*s7][64000, 1]\" = split_3[0]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_down: \"bf16[s3, 512*s7][64000, 1]\" = split_3[1]\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             dora_mag_down: \"bf16[s3, 512][64000, 1]\" = split_3[2];  split_3 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:238 in forward, code: lora_a_down.view(batch_size, self.dora_rank, self.intermediate_size),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_new_2: \"bf16[s3, s7, 2048][64000, 2048, 1]\" = lora_a_down.view(s3, l_self_dora_rank, l_self_intermediate_size);  lora_a_down = l_self_intermediate_size = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:239 in forward, code: lora_b_down.view(batch_size, self.hidden_size, self.dora_rank),\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_new_2: \"bf16[s3, 512, s7][64000, s7, 1]\" = lora_b_down.view(s3, l_self_hidden_size, l_self_dora_rank);  lora_b_down = l_self_dora_rank = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:240 in forward, code: dora_mag_down.view(batch_size, self.hidden_size)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             dora_magnitude_new_1: \"bf16[s3, 512][64000, 1]\" = dora_mag_down.view(s3, l_self_hidden_size);  dora_mag_down = l_self_hidden_size = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:141 in forward, code: base_output_unscaled = F.linear(x, weight_normalized)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output_unscaled_1: \"bf16[s3, 1, 512][512, 512, 1]\" = torch._C._nn.linear(intermediate_unsqueezed, l_norm_down_weight_);  l_norm_down_weight_ = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:142 in forward, code: base_output = base_output_unscaled * (1.0 + self.dora_magnitude_ema.unsqueeze(1))\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             unsqueeze_3: \"bf16[s3, 1, 512][64000, 512, 1]\" = dora_magnitude_new_1.unsqueeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add_3: \"bf16[s3, 1, 512][512, 512, 1]\" = 1.0 + unsqueeze_3;  unsqueeze_3 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             base_output_2: \"bf16[s3, 1, 512][512, 512, 1]\" = base_output_unscaled_1 * add_3;  base_output_unscaled_1 = add_3 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             x_reshaped_2: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = intermediate_unsqueezed.view(s3, 1, l_self_modules_down_proj_in_features);  intermediate_unsqueezed = s3 = l_self_modules_down_proj_in_features = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_a_2: \"bf16[s3, 2048, s7][64000, 1, 2048]\" = lora_a_new_2.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_b_2: \"bf16[s3, s7, 512][64000, 1, s7]\" = lora_b_new_2.transpose(1, 2)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             bmm_4: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped_2, lora_a_2);  x_reshaped_2 = lora_a_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             lora_term_2: \"bf16[s3, 1, 512][512, 512, 1]\" = torch.bmm(bmm_4, lora_b_2);  bmm_4 = lora_b_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             add_4: \"bf16[s3, 1, 512][512, 512, 1]\" = base_output_2 + lora_term_2;  base_output_2 = lora_term_2 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:242 in forward, code: down = self.down_proj(intermediate_unsqueezed, precomputed_params=precomp_down, precomputed_norm_weight=norm_down_weight).squeeze(1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             down: \"bf16[s3, 512][512, 1]\" = add_4.squeeze(1);  add_4 = None\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]             return (down, lora_a_new, lora_b_new, lora_a_new_1, lora_b_new_1, dora_magnitude_new, lora_a_new_2, lora_b_new_2, dora_magnitude_new_1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1] \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]     Original traceback:\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]       File \"/mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py\", line 211, in forward\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1] \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]     Traceback:\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]       File \"/mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py\", line 245, in forward\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1]         new_h_hyper = self.hyper_update(down, h_hyper)\n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1] \n",
      "W0810 22:02:30.677000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_1] \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2] Backend compiler exception\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]   Explanation: Backend compiler `inductor` failed with aten._local_scalar_dense.default\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2] \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]     While executing %item : [num_users=5] = call_method[target=item](args = (%getitem,), kwargs = {})\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]     GraphModule: class GraphModule(torch.nn.Module):\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         def forward(self, s3: \"Sym(s3)\", s4: \"Sym(s4)\", L_x_t_: \"f32[s3, 512][s4, 1]\", L_self_modules_fused_hyper_linear_parameters_weight_: \"f32[64000, 32][32, 1]\", L_self_modules_fused_hyper_linear_parameters_bias_: \"f32[64000][1]\", L_h_hyper_: \"f32[s3, 32][32, 1]\", L_self_buffers_fused_split_sizes_: \"i64[3][1]\", L_self_hidden_size: \"Sym(512)\", L_self_dora_rank: \"Sym(s7)\", L_self_intermediate_size: \"Sym(2048)\", L_self_modules_gate_proj_modules_linear_parameters_weight_: \"f32[2048, 512][512, 1]\", L_self_modules_gate_proj_in_features: \"Sym(512)\", L_norm_up_weight_: \"f32[2048, 512][512, 1]\", L_self_modules_up_proj_in_features: \"Sym(512)\", L_norm_down_weight_: \"f32[512, 2048][2048, 1]\", L_self_modules_down_proj_in_features: \"Sym(2048)\"):\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_x_t_ = L_x_t_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_fused_hyper_linear_parameters_weight_ = L_self_modules_fused_hyper_linear_parameters_weight_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_fused_hyper_linear_parameters_bias_ = L_self_modules_fused_hyper_linear_parameters_bias_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_h_hyper_ = L_h_hyper_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_buffers_fused_split_sizes_ = L_self_buffers_fused_split_sizes_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_hidden_size = L_self_hidden_size\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_dora_rank = L_self_dora_rank\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_intermediate_size = L_self_intermediate_size\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_gate_proj_modules_linear_parameters_weight_ = L_self_modules_gate_proj_modules_linear_parameters_weight_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_gate_proj_in_features = L_self_modules_gate_proj_in_features\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_norm_up_weight_ = L_norm_up_weight_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_up_proj_in_features = L_self_modules_up_proj_in_features\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_norm_down_weight_ = L_norm_down_weight_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_down_proj_in_features = L_self_modules_down_proj_in_features\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:207 in forward, code: x_t_unsqueezed = x_t.unsqueeze(1) # Shape: [B, 1, H]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             x_t_unsqueezed: \"f32[s3, 1, 512][s4, 512, 1]\" = l_x_t_.unsqueeze(1);  l_x_t_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:210 in forward, code: fused_params = self.fused_hyper_linear(h_hyper)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             fused_params: \"bf16[s3, 64000][64000, 1]\" = torch._C._nn.linear(l_h_hyper_, l_self_modules_fused_hyper_linear_parameters_weight_, l_self_modules_fused_hyper_linear_parameters_bias_);  l_h_hyper_ = l_self_modules_fused_hyper_linear_parameters_weight_ = l_self_modules_fused_hyper_linear_parameters_bias_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             getitem = l_self_buffers_fused_split_sizes_[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             item: \"Sym(u0)\" = getitem.item();  getitem = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _check_is_size = torch._check_is_size(item);  _check_is_size = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             ge: \"Sym(u0 >= 0)\" = item >= 0\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, \"Runtime assertion failed for expression u0 >= 0 on node 'ge'\");  ge = _assert_scalar_default = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             le: \"Sym(u0 <= 64000)\" = item <= 64000\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le, \"Runtime assertion failed for expression u0 <= 64000 on node 'le'\");  le = _assert_scalar_default_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             getitem_1 = l_self_buffers_fused_split_sizes_[1]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             item_1: \"Sym(2560*s7 + 2048)\" = getitem_1.item();  getitem_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _check_is_size_1 = torch._check_is_size(item_1);  _check_is_size_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             ge_1: \"Sym(2560*s7 + 2048 >= 4608)\" = item_1 >= 4608\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u1 >= 4608 on node 'ge_1'\");  ge_1 = _assert_scalar_default_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             le_1: \"Sym(2560*s7 + 2048 <= 64000)\" = item_1 <= 64000\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_3 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u1 <= 64000 on node 'le_1'\");  le_1 = _assert_scalar_default_3 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             eq: \"Sym(True)\" = item_1 == item_1\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_4 = torch.ops.aten._assert_scalar.default(eq, \"Runtime assertion failed for expression Eq(2560*s7 + 2048, u1) on node 'eq'\");  eq = _assert_scalar_default_4 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             getitem_2 = l_self_buffers_fused_split_sizes_[2];  l_self_buffers_fused_split_sizes_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             item_2: \"Sym(2560*s7 + 512)\" = getitem_2.item();  getitem_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _check_is_size_2 = torch._check_is_size(item_2);  _check_is_size_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             ge_2: \"Sym(2560*s7 + 512 >= 3072)\" = item_2 >= 3072\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_5 = torch.ops.aten._assert_scalar.default(ge_2, \"Runtime assertion failed for expression u2 >= 3072 on node 'ge_2'\");  ge_2 = _assert_scalar_default_5 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             le_2: \"Sym(2560*s7 + 512 <= 61952)\" = item_2 <= 61952\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_6 = torch.ops.aten._assert_scalar.default(le_2, \"Runtime assertion failed for expression u2 <= 61952 on node 'le_2'\");  le_2 = _assert_scalar_default_6 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             sym_sum: \"Sym(5120*s7 + u0 + 2560)\" = torch.sym_sum([item, item_1, item_2])\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             eq_1: \"Sym(Eq(5120*s7 + u0 + 2560, 64000))\" = sym_sum == 64000;  sym_sum = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_7 = torch.ops.aten._assert_scalar.default(eq_1, \"Runtime assertion failed for expression Eq(u0 + u1 + u2, 64000) on node 'eq_1'\");  eq_1 = _assert_scalar_default_7 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             eq_2: \"Sym(True)\" = item_2 == item_2\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_8 = torch.ops.aten._assert_scalar.default(eq_2, \"Runtime assertion failed for expression Eq(2560*s7 + 512, u2) on node 'eq_2'\");  eq_2 = _assert_scalar_default_8 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             split = torch.functional.split(fused_params, [item, item_1, item_2], dim = -1);  fused_params = item = item_1 = item_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             gate_params: \"bf16[s3, u0][64000, 1]\" = split[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             up_params: \"bf16[s3, 2560*s7 + 2048][64000, 1]\" = split[1]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             down_params: \"bf16[s3, 2560*s7 + 512][64000, 1]\" = split[2];  split = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:214 in forward, code: lora_a_gate, lora_b_gate = gate_params.split([self.hidden_size * self.dora_rank, self.intermediate_size * self.dora_rank], dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             mul: \"Sym(512*s7)\" = l_self_hidden_size * l_self_dora_rank\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             mul_1: \"Sym(2048*s7)\" = l_self_intermediate_size * l_self_dora_rank\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             split_1 = gate_params.split([mul, mul_1], dim = -1);  gate_params = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_gate: \"bf16[s3, 512*s7][64000, 1]\" = split_1[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_gate: \"bf16[s3, 2048*s7][64000, 1]\" = split_1[1];  split_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:216 in forward, code: lora_a_gate.view(batch_size, self.dora_rank, self.hidden_size),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_new: \"bf16[s3, s7, 512][64000, 512, 1]\" = lora_a_gate.view(s3, l_self_dora_rank, l_self_hidden_size);  lora_a_gate = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:217 in forward, code: lora_b_gate.view(batch_size, self.intermediate_size, self.dora_rank),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_new: \"bf16[s3, 2048, s7][64000, s7, 1]\" = lora_b_gate.view(s3, l_self_intermediate_size, l_self_dora_rank);  lora_b_gate = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:130 in forward, code: base_output = self.linear(x)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch._C._nn.linear(x_t_unsqueezed, l_self_modules_gate_proj_modules_linear_parameters_weight_, None);  l_self_modules_gate_proj_modules_linear_parameters_weight_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             x_reshaped: \"f32[s3, 1, 512][s4, 512, 1]\" = x_t_unsqueezed.view(s3, 1, l_self_modules_gate_proj_in_features);  l_self_modules_gate_proj_in_features = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a: \"bf16[s3, 512, s7][64000, 1, 512]\" = lora_a_new.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b: \"bf16[s3, s7, 2048][64000, 1, s7]\" = lora_b_new.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             bmm: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped, lora_a);  x_reshaped = lora_a = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_term: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch.bmm(bmm, lora_b);  bmm = lora_b = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output + lora_term;  base_output = lora_term = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:220 in forward, code: gate = self.gate_proj(x_t_unsqueezed, precomputed_params=precomp_gate).squeeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             gate: \"bf16[s3, 2048][2048, 1]\" = add.squeeze(1);  add = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:223 in forward, code: lora_a_up, lora_b_up, dora_mag_up = up_params.split([self.hidden_size * self.dora_rank, self.intermediate_size * self.dora_rank, self.intermediate_size], dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             split_2 = up_params.split([mul, mul_1, l_self_intermediate_size], dim = -1);  up_params = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_up: \"bf16[s3, 512*s7][64000, 1]\" = split_2[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_up: \"bf16[s3, 2048*s7][64000, 1]\" = split_2[1]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             dora_mag_up: \"bf16[s3, 2048][64000, 1]\" = split_2[2];  split_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:225 in forward, code: lora_a_up.view(batch_size, self.dora_rank, self.hidden_size),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_new_1: \"bf16[s3, s7, 512][64000, 512, 1]\" = lora_a_up.view(s3, l_self_dora_rank, l_self_hidden_size);  lora_a_up = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:226 in forward, code: lora_b_up.view(batch_size, self.intermediate_size, self.dora_rank),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_new_1: \"bf16[s3, 2048, s7][64000, s7, 1]\" = lora_b_up.view(s3, l_self_intermediate_size, l_self_dora_rank);  lora_b_up = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:227 in forward, code: dora_mag_up.view(batch_size, self.intermediate_size)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             dora_magnitude_new: \"bf16[s3, 2048][64000, 1]\" = dora_mag_up.view(s3, l_self_intermediate_size);  dora_mag_up = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:141 in forward, code: base_output_unscaled = F.linear(x, weight_normalized)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output_unscaled: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch._C._nn.linear(x_t_unsqueezed, l_norm_up_weight_);  l_norm_up_weight_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:142 in forward, code: base_output = base_output_unscaled * (1.0 + self.dora_magnitude_ema.unsqueeze(1))\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             unsqueeze_1: \"bf16[s3, 1, 2048][64000, 2048, 1]\" = dora_magnitude_new.unsqueeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = 1.0 + unsqueeze_1;  unsqueeze_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output_unscaled * add_1;  base_output_unscaled = add_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             x_reshaped_1: \"f32[s3, 1, 512][s4, 512, 1]\" = x_t_unsqueezed.view(s3, 1, l_self_modules_up_proj_in_features);  x_t_unsqueezed = l_self_modules_up_proj_in_features = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_1: \"bf16[s3, 512, s7][64000, 1, 512]\" = lora_a_new_1.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_1: \"bf16[s3, s7, 2048][64000, 1, s7]\" = lora_b_new_1.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             bmm_2: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped_1, lora_a_1);  x_reshaped_1 = lora_a_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_term_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch.bmm(bmm_2, lora_b_1);  bmm_2 = lora_b_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add_2: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output_1 + lora_term_1;  base_output_1 = lora_term_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:229 in forward, code: up = self.up_proj(x_t_unsqueezed, precomputed_params=precomp_up, precomputed_norm_weight=norm_up_weight).squeeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             up: \"bf16[s3, 2048][2048, 1]\" = add_2.squeeze(1);  add_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:232 in forward, code: intermediate = self.act_fn(gate) * up\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             silu: \"bf16[s3, 2048][2048, 1]\" = torch.nn.functional.silu(gate, inplace = False);  gate = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             intermediate: \"bf16[s3, 2048][2048, 1]\" = silu * up;  silu = up = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:233 in forward, code: intermediate_unsqueezed = intermediate.unsqueeze(1) # Shape: [B, 1, I]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             intermediate_unsqueezed: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = intermediate.unsqueeze(1);  intermediate = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:236 in forward, code: lora_a_down, lora_b_down, dora_mag_down = down_params.split([self.intermediate_size * self.dora_rank, self.hidden_size * self.dora_rank, self.hidden_size], dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             split_3 = down_params.split([mul_1, mul, l_self_hidden_size], dim = -1);  down_params = mul_1 = mul = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_down: \"bf16[s3, 2048*s7][64000, 1]\" = split_3[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_down: \"bf16[s3, 512*s7][64000, 1]\" = split_3[1]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             dora_mag_down: \"bf16[s3, 512][64000, 1]\" = split_3[2];  split_3 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:238 in forward, code: lora_a_down.view(batch_size, self.dora_rank, self.intermediate_size),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_new_2: \"bf16[s3, s7, 2048][64000, 2048, 1]\" = lora_a_down.view(s3, l_self_dora_rank, l_self_intermediate_size);  lora_a_down = l_self_intermediate_size = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:239 in forward, code: lora_b_down.view(batch_size, self.hidden_size, self.dora_rank),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_new_2: \"bf16[s3, 512, s7][64000, s7, 1]\" = lora_b_down.view(s3, l_self_hidden_size, l_self_dora_rank);  lora_b_down = l_self_dora_rank = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:240 in forward, code: dora_mag_down.view(batch_size, self.hidden_size)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             dora_magnitude_new_1: \"bf16[s3, 512][64000, 1]\" = dora_mag_down.view(s3, l_self_hidden_size);  dora_mag_down = l_self_hidden_size = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:141 in forward, code: base_output_unscaled = F.linear(x, weight_normalized)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output_unscaled_1: \"bf16[s3, 1, 512][512, 512, 1]\" = torch._C._nn.linear(intermediate_unsqueezed, l_norm_down_weight_);  l_norm_down_weight_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:142 in forward, code: base_output = base_output_unscaled * (1.0 + self.dora_magnitude_ema.unsqueeze(1))\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             unsqueeze_3: \"bf16[s3, 1, 512][64000, 512, 1]\" = dora_magnitude_new_1.unsqueeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add_3: \"bf16[s3, 1, 512][512, 512, 1]\" = 1.0 + unsqueeze_3;  unsqueeze_3 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output_2: \"bf16[s3, 1, 512][512, 512, 1]\" = base_output_unscaled_1 * add_3;  base_output_unscaled_1 = add_3 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             x_reshaped_2: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = intermediate_unsqueezed.view(s3, 1, l_self_modules_down_proj_in_features);  intermediate_unsqueezed = s3 = l_self_modules_down_proj_in_features = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_2: \"bf16[s3, 2048, s7][64000, 1, 2048]\" = lora_a_new_2.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_2: \"bf16[s3, s7, 512][64000, 1, s7]\" = lora_b_new_2.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             bmm_4: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped_2, lora_a_2);  x_reshaped_2 = lora_a_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_term_2: \"bf16[s3, 1, 512][512, 512, 1]\" = torch.bmm(bmm_4, lora_b_2);  bmm_4 = lora_b_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add_4: \"bf16[s3, 1, 512][512, 512, 1]\" = base_output_2 + lora_term_2;  base_output_2 = lora_term_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:242 in forward, code: down = self.down_proj(intermediate_unsqueezed, precomputed_params=precomp_down, precomputed_norm_weight=norm_down_weight).squeeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             down: \"bf16[s3, 512][512, 1]\" = add_4.squeeze(1);  add_4 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             return (down, lora_a_new, lora_b_new, lora_a_new_1, lora_b_new_1, dora_magnitude_new, lora_a_new_2, lora_b_new_2, dora_magnitude_new_1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2] \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]     Original traceback:\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]       File \"/mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py\", line 211, in forward\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]     . Adding a graph break.\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]   Hint: Report an issue to the backend compiler repo.\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2] \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]   Developer debug context: Backend: inductor\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]     Exception:aten._local_scalar_dense.default\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2] \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]     While executing %item : [num_users=5] = call_method[target=item](args = (%getitem,), kwargs = {})\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]     GraphModule: class GraphModule(torch.nn.Module):\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         def forward(self, s3: \"Sym(s3)\", s4: \"Sym(s4)\", L_x_t_: \"f32[s3, 512][s4, 1]\", L_self_modules_fused_hyper_linear_parameters_weight_: \"f32[64000, 32][32, 1]\", L_self_modules_fused_hyper_linear_parameters_bias_: \"f32[64000][1]\", L_h_hyper_: \"f32[s3, 32][32, 1]\", L_self_buffers_fused_split_sizes_: \"i64[3][1]\", L_self_hidden_size: \"Sym(512)\", L_self_dora_rank: \"Sym(s7)\", L_self_intermediate_size: \"Sym(2048)\", L_self_modules_gate_proj_modules_linear_parameters_weight_: \"f32[2048, 512][512, 1]\", L_self_modules_gate_proj_in_features: \"Sym(512)\", L_norm_up_weight_: \"f32[2048, 512][512, 1]\", L_self_modules_up_proj_in_features: \"Sym(512)\", L_norm_down_weight_: \"f32[512, 2048][2048, 1]\", L_self_modules_down_proj_in_features: \"Sym(2048)\"):\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_x_t_ = L_x_t_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_fused_hyper_linear_parameters_weight_ = L_self_modules_fused_hyper_linear_parameters_weight_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_fused_hyper_linear_parameters_bias_ = L_self_modules_fused_hyper_linear_parameters_bias_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_h_hyper_ = L_h_hyper_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_buffers_fused_split_sizes_ = L_self_buffers_fused_split_sizes_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_hidden_size = L_self_hidden_size\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_dora_rank = L_self_dora_rank\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_intermediate_size = L_self_intermediate_size\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_gate_proj_modules_linear_parameters_weight_ = L_self_modules_gate_proj_modules_linear_parameters_weight_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_gate_proj_in_features = L_self_modules_gate_proj_in_features\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_norm_up_weight_ = L_norm_up_weight_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_up_proj_in_features = L_self_modules_up_proj_in_features\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_norm_down_weight_ = L_norm_down_weight_\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             l_self_modules_down_proj_in_features = L_self_modules_down_proj_in_features\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:207 in forward, code: x_t_unsqueezed = x_t.unsqueeze(1) # Shape: [B, 1, H]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             x_t_unsqueezed: \"f32[s3, 1, 512][s4, 512, 1]\" = l_x_t_.unsqueeze(1);  l_x_t_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:210 in forward, code: fused_params = self.fused_hyper_linear(h_hyper)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             fused_params: \"bf16[s3, 64000][64000, 1]\" = torch._C._nn.linear(l_h_hyper_, l_self_modules_fused_hyper_linear_parameters_weight_, l_self_modules_fused_hyper_linear_parameters_bias_);  l_h_hyper_ = l_self_modules_fused_hyper_linear_parameters_weight_ = l_self_modules_fused_hyper_linear_parameters_bias_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             getitem = l_self_buffers_fused_split_sizes_[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             item: \"Sym(u0)\" = getitem.item();  getitem = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _check_is_size = torch._check_is_size(item);  _check_is_size = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             ge: \"Sym(u0 >= 0)\" = item >= 0\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, \"Runtime assertion failed for expression u0 >= 0 on node 'ge'\");  ge = _assert_scalar_default = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             le: \"Sym(u0 <= 64000)\" = item <= 64000\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le, \"Runtime assertion failed for expression u0 <= 64000 on node 'le'\");  le = _assert_scalar_default_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             getitem_1 = l_self_buffers_fused_split_sizes_[1]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             item_1: \"Sym(2560*s7 + 2048)\" = getitem_1.item();  getitem_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _check_is_size_1 = torch._check_is_size(item_1);  _check_is_size_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             ge_1: \"Sym(2560*s7 + 2048 >= 4608)\" = item_1 >= 4608\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u1 >= 4608 on node 'ge_1'\");  ge_1 = _assert_scalar_default_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             le_1: \"Sym(2560*s7 + 2048 <= 64000)\" = item_1 <= 64000\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_3 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u1 <= 64000 on node 'le_1'\");  le_1 = _assert_scalar_default_3 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             eq: \"Sym(True)\" = item_1 == item_1\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_4 = torch.ops.aten._assert_scalar.default(eq, \"Runtime assertion failed for expression Eq(2560*s7 + 2048, u1) on node 'eq'\");  eq = _assert_scalar_default_4 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             getitem_2 = l_self_buffers_fused_split_sizes_[2];  l_self_buffers_fused_split_sizes_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             item_2: \"Sym(2560*s7 + 512)\" = getitem_2.item();  getitem_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _check_is_size_2 = torch._check_is_size(item_2);  _check_is_size_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             ge_2: \"Sym(2560*s7 + 512 >= 3072)\" = item_2 >= 3072\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_5 = torch.ops.aten._assert_scalar.default(ge_2, \"Runtime assertion failed for expression u2 >= 3072 on node 'ge_2'\");  ge_2 = _assert_scalar_default_5 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             le_2: \"Sym(2560*s7 + 512 <= 61952)\" = item_2 <= 61952\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_6 = torch.ops.aten._assert_scalar.default(le_2, \"Runtime assertion failed for expression u2 <= 61952 on node 'le_2'\");  le_2 = _assert_scalar_default_6 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             # No stacktrace found for following nodes\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             sym_sum: \"Sym(5120*s7 + u0 + 2560)\" = torch.sym_sum([item, item_1, item_2])\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             eq_1: \"Sym(Eq(5120*s7 + u0 + 2560, 64000))\" = sym_sum == 64000;  sym_sum = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_7 = torch.ops.aten._assert_scalar.default(eq_1, \"Runtime assertion failed for expression Eq(u0 + u1 + u2, 64000) on node 'eq_1'\");  eq_1 = _assert_scalar_default_7 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             eq_2: \"Sym(True)\" = item_2 == item_2\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             _assert_scalar_default_8 = torch.ops.aten._assert_scalar.default(eq_2, \"Runtime assertion failed for expression Eq(2560*s7 + 512, u2) on node 'eq_2'\");  eq_2 = _assert_scalar_default_8 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:211 in forward, code: gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             split = torch.functional.split(fused_params, [item, item_1, item_2], dim = -1);  fused_params = item = item_1 = item_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             gate_params: \"bf16[s3, u0][64000, 1]\" = split[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             up_params: \"bf16[s3, 2560*s7 + 2048][64000, 1]\" = split[1]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             down_params: \"bf16[s3, 2560*s7 + 512][64000, 1]\" = split[2];  split = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:214 in forward, code: lora_a_gate, lora_b_gate = gate_params.split([self.hidden_size * self.dora_rank, self.intermediate_size * self.dora_rank], dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             mul: \"Sym(512*s7)\" = l_self_hidden_size * l_self_dora_rank\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             mul_1: \"Sym(2048*s7)\" = l_self_intermediate_size * l_self_dora_rank\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             split_1 = gate_params.split([mul, mul_1], dim = -1);  gate_params = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_gate: \"bf16[s3, 512*s7][64000, 1]\" = split_1[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_gate: \"bf16[s3, 2048*s7][64000, 1]\" = split_1[1];  split_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:216 in forward, code: lora_a_gate.view(batch_size, self.dora_rank, self.hidden_size),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_new: \"bf16[s3, s7, 512][64000, 512, 1]\" = lora_a_gate.view(s3, l_self_dora_rank, l_self_hidden_size);  lora_a_gate = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:217 in forward, code: lora_b_gate.view(batch_size, self.intermediate_size, self.dora_rank),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_new: \"bf16[s3, 2048, s7][64000, s7, 1]\" = lora_b_gate.view(s3, l_self_intermediate_size, l_self_dora_rank);  lora_b_gate = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:130 in forward, code: base_output = self.linear(x)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch._C._nn.linear(x_t_unsqueezed, l_self_modules_gate_proj_modules_linear_parameters_weight_, None);  l_self_modules_gate_proj_modules_linear_parameters_weight_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             x_reshaped: \"f32[s3, 1, 512][s4, 512, 1]\" = x_t_unsqueezed.view(s3, 1, l_self_modules_gate_proj_in_features);  l_self_modules_gate_proj_in_features = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a: \"bf16[s3, 512, s7][64000, 1, 512]\" = lora_a_new.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b: \"bf16[s3, s7, 2048][64000, 1, s7]\" = lora_b_new.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             bmm: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped, lora_a);  x_reshaped = lora_a = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_term: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch.bmm(bmm, lora_b);  bmm = lora_b = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output + lora_term;  base_output = lora_term = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:220 in forward, code: gate = self.gate_proj(x_t_unsqueezed, precomputed_params=precomp_gate).squeeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             gate: \"bf16[s3, 2048][2048, 1]\" = add.squeeze(1);  add = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:223 in forward, code: lora_a_up, lora_b_up, dora_mag_up = up_params.split([self.hidden_size * self.dora_rank, self.intermediate_size * self.dora_rank, self.intermediate_size], dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             split_2 = up_params.split([mul, mul_1, l_self_intermediate_size], dim = -1);  up_params = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_up: \"bf16[s3, 512*s7][64000, 1]\" = split_2[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_up: \"bf16[s3, 2048*s7][64000, 1]\" = split_2[1]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             dora_mag_up: \"bf16[s3, 2048][64000, 1]\" = split_2[2];  split_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:225 in forward, code: lora_a_up.view(batch_size, self.dora_rank, self.hidden_size),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_new_1: \"bf16[s3, s7, 512][64000, 512, 1]\" = lora_a_up.view(s3, l_self_dora_rank, l_self_hidden_size);  lora_a_up = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:226 in forward, code: lora_b_up.view(batch_size, self.intermediate_size, self.dora_rank),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_new_1: \"bf16[s3, 2048, s7][64000, s7, 1]\" = lora_b_up.view(s3, l_self_intermediate_size, l_self_dora_rank);  lora_b_up = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:227 in forward, code: dora_mag_up.view(batch_size, self.intermediate_size)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             dora_magnitude_new: \"bf16[s3, 2048][64000, 1]\" = dora_mag_up.view(s3, l_self_intermediate_size);  dora_mag_up = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:141 in forward, code: base_output_unscaled = F.linear(x, weight_normalized)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output_unscaled: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch._C._nn.linear(x_t_unsqueezed, l_norm_up_weight_);  l_norm_up_weight_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:142 in forward, code: base_output = base_output_unscaled * (1.0 + self.dora_magnitude_ema.unsqueeze(1))\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             unsqueeze_1: \"bf16[s3, 1, 2048][64000, 2048, 1]\" = dora_magnitude_new.unsqueeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = 1.0 + unsqueeze_1;  unsqueeze_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output_unscaled * add_1;  base_output_unscaled = add_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             x_reshaped_1: \"f32[s3, 1, 512][s4, 512, 1]\" = x_t_unsqueezed.view(s3, 1, l_self_modules_up_proj_in_features);  x_t_unsqueezed = l_self_modules_up_proj_in_features = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_1: \"bf16[s3, 512, s7][64000, 1, 512]\" = lora_a_new_1.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_1: \"bf16[s3, s7, 2048][64000, 1, s7]\" = lora_b_new_1.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             bmm_2: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped_1, lora_a_1);  x_reshaped_1 = lora_a_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_term_1: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = torch.bmm(bmm_2, lora_b_1);  bmm_2 = lora_b_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add_2: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = base_output_1 + lora_term_1;  base_output_1 = lora_term_1 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:229 in forward, code: up = self.up_proj(x_t_unsqueezed, precomputed_params=precomp_up, precomputed_norm_weight=norm_up_weight).squeeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             up: \"bf16[s3, 2048][2048, 1]\" = add_2.squeeze(1);  add_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:232 in forward, code: intermediate = self.act_fn(gate) * up\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             silu: \"bf16[s3, 2048][2048, 1]\" = torch.nn.functional.silu(gate, inplace = False);  gate = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             intermediate: \"bf16[s3, 2048][2048, 1]\" = silu * up;  silu = up = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:233 in forward, code: intermediate_unsqueezed = intermediate.unsqueeze(1) # Shape: [B, 1, I]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             intermediate_unsqueezed: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = intermediate.unsqueeze(1);  intermediate = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:236 in forward, code: lora_a_down, lora_b_down, dora_mag_down = down_params.split([self.intermediate_size * self.dora_rank, self.hidden_size * self.dora_rank, self.hidden_size], dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             split_3 = down_params.split([mul_1, mul, l_self_hidden_size], dim = -1);  down_params = mul_1 = mul = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_down: \"bf16[s3, 2048*s7][64000, 1]\" = split_3[0]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_down: \"bf16[s3, 512*s7][64000, 1]\" = split_3[1]\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             dora_mag_down: \"bf16[s3, 512][64000, 1]\" = split_3[2];  split_3 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:238 in forward, code: lora_a_down.view(batch_size, self.dora_rank, self.intermediate_size),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_new_2: \"bf16[s3, s7, 2048][64000, 2048, 1]\" = lora_a_down.view(s3, l_self_dora_rank, l_self_intermediate_size);  lora_a_down = l_self_intermediate_size = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:239 in forward, code: lora_b_down.view(batch_size, self.hidden_size, self.dora_rank),\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_new_2: \"bf16[s3, 512, s7][64000, s7, 1]\" = lora_b_down.view(s3, l_self_hidden_size, l_self_dora_rank);  lora_b_down = l_self_dora_rank = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:240 in forward, code: dora_mag_down.view(batch_size, self.hidden_size)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             dora_magnitude_new_1: \"bf16[s3, 512][64000, 1]\" = dora_mag_down.view(s3, l_self_hidden_size);  dora_mag_down = l_self_hidden_size = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:141 in forward, code: base_output_unscaled = F.linear(x, weight_normalized)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output_unscaled_1: \"bf16[s3, 1, 512][512, 512, 1]\" = torch._C._nn.linear(intermediate_unsqueezed, l_norm_down_weight_);  l_norm_down_weight_ = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:142 in forward, code: base_output = base_output_unscaled * (1.0 + self.dora_magnitude_ema.unsqueeze(1))\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             unsqueeze_3: \"bf16[s3, 1, 512][64000, 512, 1]\" = dora_magnitude_new_1.unsqueeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add_3: \"bf16[s3, 1, 512][512, 512, 1]\" = 1.0 + unsqueeze_3;  unsqueeze_3 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             base_output_2: \"bf16[s3, 1, 512][512, 512, 1]\" = base_output_unscaled_1 * add_3;  base_output_unscaled_1 = add_3 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:145 in forward, code: x_reshaped = x.view(batch_size, seq_len, self.in_features)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             x_reshaped_2: \"bf16[s3, 1, 2048][2048, 2048, 1]\" = intermediate_unsqueezed.view(s3, 1, l_self_modules_down_proj_in_features);  intermediate_unsqueezed = s3 = l_self_modules_down_proj_in_features = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:146 in forward, code: lora_a = self.lora_a_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_a_2: \"bf16[s3, 2048, s7][64000, 1, 2048]\" = lora_a_new_2.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:147 in forward, code: lora_b = self.lora_b_ema.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_b_2: \"bf16[s3, s7, 512][64000, 1, s7]\" = lora_b_new_2.transpose(1, 2)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:149 in forward, code: lora_term = torch.bmm(torch.bmm(x_reshaped, lora_a), lora_b)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             bmm_4: \"bf16[s3, 1, s7][s7, s7, 1]\" = torch.bmm(x_reshaped_2, lora_a_2);  x_reshaped_2 = lora_a_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             lora_term_2: \"bf16[s3, 1, 512][512, 512, 1]\" = torch.bmm(bmm_4, lora_b_2);  bmm_4 = lora_b_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:151 in forward, code: return base_output + lora_term\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             add_4: \"bf16[s3, 1, 512][512, 512, 1]\" = base_output_2 + lora_term_2;  base_output_2 = lora_term_2 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]              # File: /mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:242 in forward, code: down = self.down_proj(intermediate_unsqueezed, precomputed_params=precomp_down, precomputed_norm_weight=norm_down_weight).squeeze(1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             down: \"bf16[s3, 512][512, 1]\" = add_4.squeeze(1);  add_4 = None\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]             return (down, lora_a_new, lora_b_new, lora_a_new_1, lora_b_new_1, dora_magnitude_new, lora_a_new_2, lora_b_new_2, dora_magnitude_new_1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2] \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]     Original traceback:\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]       File \"/mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py\", line 211, in forward\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         gate_params, up_params, down_params = torch.split(fused_params, self.fused_split_sizes.tolist(), dim=-1)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2] \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]     Traceback:\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]       File \"/mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py\", line 245, in forward\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2]         new_h_hyper = self.hyper_update(down, h_hyper)\n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2] \n",
      "W0810 22:02:30.857000 79840 site-packages/torch/_dynamo/exc.py:514] [7/0_2] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681b4a4e639b4b69af545fd8893dac4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                    | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0810 22:02:40.575000 79840 site-packages/torch/_dynamo/convert_frame.py:964] [8/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0810 22:02:40.575000 79840 site-packages/torch/_dynamo/convert_frame.py:964] [8/8]    function: 'forward' (/mnt/c/Users/gabol/Desktop/ArchiFactory/private/hyper_modules.py:65)\n",
      "W0810 22:02:40.575000 79840 site-packages/torch/_dynamo/convert_frame.py:964] [8/8]    last reason: 8/7: tensor 'x' dtype mismatch. expected Float, actual BFloat16\n",
      "W0810 22:02:40.575000 79840 site-packages/torch/_dynamo/convert_frame.py:964] [8/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0810 22:02:40.575000 79840 site-packages/torch/_dynamo/convert_frame.py:964] [8/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from private.hyper_modules import RecurrentHyperLayer#, ParallelScanRecurrentLayer\n",
    "\n",
    "rhn = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=None,\n",
    "    ffn_module=RecurrentHyperLayer(\n",
    "        hidden_size=hidden_size,\n",
    "        hyper_features=32,\n",
    "        intermediate_size=intermediate_size,\n",
    "        dora_rank=8,\n",
    "        ema_steps = 0,\n",
    "    ),\n",
    "    positionnal_module=NaivePositionnalEmbedding(hidden_size, max_position_embeddings=max_position_embeddings)\n",
    ")\n",
    "\n",
    "count_parameters(rhn)\n",
    "train(rhn,run_name='rhn', do_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b54ae-2625-4f36-96e2-44d810046633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad5c50-8cfd-4bc8-a540-2bcf5c0c2b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875ac82-b9a2-4ca5-8b57-76d6acb4bfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c5f54-2bd2-4d0b-88ab-ef9f124fa769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c7b86-6acd-4c2f-9baf-c858e0ace319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560d2be-1b62-48aa-8843-0856b25c0e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc269d6d-9dcd-4d88-b13c-b0e606021ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ce1e6-9373-450b-b523-3ca973746883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e6ac5-606d-4f10-8734-8b03a6e684b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429ed44-8b4c-4aeb-b038-53991358214d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508d59e-515e-4343-83c7-9a28b88795ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd03c60-1941-426b-ab61-f959c19f7b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06b981c5",
   "metadata": {},
   "source": [
    "## STACK 8 - Moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9471ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 6\n",
    "ffn_module = SparseMoeFFN(\n",
    "    hidden_size,\n",
    "    hidden_size*4,\n",
    "    num_experts=8,\n",
    "    num_experts_per_tok=2,\n",
    "    norm_topk_prob=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b43ba",
   "metadata": {},
   "source": [
    "### GQA MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gqsa_moe = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=GroupedQuerySelfAttentionMixin(hidden_size, num_attention_heads=9, num_key_value_heads=9),\n",
    "    ffn_module=ffn_module,\n",
    "    positionnal_module=NaivePositionnalEmbedding(hidden_size, max_length=max_length)\n",
    ")\n",
    "\n",
    "count_parameters(gqsa_moe)\n",
    "train(gqsa_moe,run_name='gqsa-moe', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd0af3",
   "metadata": {},
   "source": [
    "### Retentive Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "retnet_moe = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=MultiScaleRetentionMixin(hidden_size, num_attention_heads=9, num_key_value_heads=3),\n",
    "    ffn_module=ffn_module,\n",
    "    # positionnal_module=NaivePositionnalEmbedding(hidden_size, max_length=max_length)\n",
    ")\n",
    "\n",
    "count_parameters(retnet_moe)\n",
    "train(retnet_moe,run_name='retnet-moe', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cc33b",
   "metadata": {},
   "source": [
    "### Mamba MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mamba_moe = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=Mamba2Mixin(hidden_size = hidden_size, num_attention_heads=6),\n",
    "    ffn_module=ffn_module,\n",
    "    # positionnal_module=NaivePositionnalEmbedding(hidden_size, max_length=max_length)\n",
    ")\n",
    "\n",
    "count_parameters(mamba_moe)\n",
    "train(mamba_moe,run_name='mamba-moe', do_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ab488",
   "metadata": {},
   "source": [
    "### RWKV MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rwkv_moe = StackedMixinForCausalLM(\n",
    "    num_layers=num_layers,\n",
    "    hidden_size=hidden_size,\n",
    "    initializer_range=0.02,\n",
    "    embedding_module=embed_tokens,\n",
    "    lm_head_module=lm_head,\n",
    "    final_norm_module=norm,\n",
    "    freeze_lm_modules=False,\n",
    "    vocab_size=vocab_size,\n",
    "    mixin_module=RWKV6Mixin(hidden_size = hidden_size, num_attention_heads=9),\n",
    "    ffn_module=ffn_module,\n",
    "    # positionnal_module=NaivePositionnalEmbedding(hidden_size, max_length=max_length)\n",
    ")\n",
    "\n",
    "count_parameters(rwkv_moe)\n",
    "train(rwkv_moe,run_name='rwkv-moe', do_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3caec-54d3-46d1-94cb-e5096b687f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae90173-c7d2-4961-bd59-7632001cba02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e917bc-4467-435e-b833-40250f033cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
